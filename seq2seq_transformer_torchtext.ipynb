{"cells":[{"cell_type":"markdown","source":["## Accelerated Training of Sequence-to-Sequence Transformer Model\nThis notebook demonstrates using [ONNX Runtime](https://cloudblogs.microsoft.com/opensource/2020/05/19/announcing-support-for-accelerated-training-with-onnx-runtime/) to accelerate the training of a simple sequence-to-sequence model. It uses a slightly modified version of the implementation available at [PyTorch tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) on [nn.Transformer](https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer) module. This example uses [Adam](https://arxiv.org/abs/1412.6980) optimization instead of [SGD](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) that is used in the original implementation. This notebook is available at [https://github.com/skaarthik/onnxruntime-training-databricks](https://github.com/skaarthik/onnxruntime-training-databricks). Recipes on using ONNX Runtime to accelerate pretraining and finetuning of BERT and GPT-2 models are available at [https://github.com/microsoft/onnxruntime-training-examples](https://github.com/microsoft/onnxruntime-training-examples)."],"metadata":{}},{"cell_type":"markdown","source":["####Prerequisites for the demo\nDatabricks cluster:\n* Databricks cluster with `7.3 LTS ML` runtime\n* Node with `V100` GPU (like, `Standard_NC6s_v3` node in Azure)\n\nCustom containers are not supported on ML Runtime in Databricks. This demo needs ML Runtime and GPUs. As an alternative to packaging dependencies in a customer container, use the commands below to prepare Databricks environment for this notebook. It is an one-time process for a given Databricks cluster.\n\nRun the following commands to install the following packages to the Conda environment you will be using for this demo:\n* pip install `torchtext`\n* pip install `onnx`\n* pip install `cerberus`\n* pip install `https://onnxtraining.blob.core.windows.net/ort-databricks-demo/onnxruntime_gpu-1.5.1-cp37-cp37m-linux_x86_64.whl`"],"metadata":{}},{"cell_type":"markdown","source":["####Define the model\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model for details."],"metadata":{}},{"cell_type":"code","source":["import math\nimport torch\nimport torch.nn as nn\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\nclass TransformerModel(nn.Module):\n\n    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n        super(TransformerModel, self).__init__()\n        self.model_type = 'Transformer'\n        self.src_mask = None\n        self.pos_encoder = PositionalEncoding(ninp, dropout)\n        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        self.ninp = ninp\n        self.decoder = nn.Linear(ninp, ntoken)\n\n        self.init_weights()\n\n    def _generate_square_subsequent_mask(self, src):\n        sz = src.size(0)\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src):\n        if self.src_mask == None or self.src_mask.size(0) != src.size(0):\n            device = src.device\n            mask = self._generate_square_subsequent_mask(src).to(device)\n            self.src_mask = mask\n\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, self.src_mask)\n        output = self.decoder(output)\n        return output"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["####Load and batch data\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#load-and-batch-data for details."],"metadata":{}},{"cell_type":"code","source":["import torchtext\nfrom torchtext.data.utils import get_tokenizer\nTEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n                            init_token='<sos>',\n                            eos_token='<eos>',\n                            lower=True)\ntrain_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\nTEXT.build_vocab(train_txt)\ndevice = torch.device(\"cuda\")"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["####Functions to generate input and target sequence\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#functions-to-generate-input-and-target-sequence for details."],"metadata":{}},{"cell_type":"code","source":["bptt = 35\ndef get_batch(source, i):\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].view(-1)\n    return data, target\n  \ndef batchify(data, bsz):\n    data = TEXT.numericalize([data.examples[0].text])\n    # Divide the dataset into bsz parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    data = data.view(bsz, -1).t().contiguous()\n    return data.to(device)\n\nbatch_size = 20\neval_batch_size = 10\ntrain_data = batchify(train_txt, batch_size)\nval_data = batchify(val_txt, eval_batch_size)\ntest_data = batchify(test_txt, eval_batch_size)  "],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["####Initialize variables needed for model creation\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#initiate-an-instance for details."],"metadata":{}},{"cell_type":"code","source":["ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\nemsize = 200 # embedding dimension\nnhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\nnlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\nnhead = 2 # the number of heads in the multiheadattention models\ndropout = 0.2 # the dropout value\nlr = 0.001 # learning rate\n\ndef calculate_loss(output, targets):\n    output = output.view(-1, len(TEXT.vocab.stoi))\n    return criterion(output, targets) "],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["####Initiate an instance of the model for training `without ONNX Runtime acceleration` (referred to as `baseline` training in this notebook)\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#initiate-an-instance for details.\n\nAfter the next few steps below that are required for `baseline` training, this notebook will cover the steps required for accelerated training using ONNX Runtime (and that is referred to as `ort` training)."],"metadata":{}},{"cell_type":"code","source":["model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\ncriterion = nn.CrossEntropyLoss()\nlearning_rate = lr\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nmetric_prefix = 'baseline'  "],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["####Define function to use mlflow logging"],"metadata":{}},{"cell_type":"code","source":["import mlflow\ndef log_metrics(epoch, batch, train_data_len, bptt, lr, elapsed, log_interval, cur_loss, log_prefix):\n  ms = elapsed * 1000 / log_interval\n  ppl = math.exp(cur_loss)\n  print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.3f} | ms/batch {:5.2f} | loss {:5.2f} | ppl {:8.2f}'.\n        format(epoch, batch, train_data_len // bptt, lr, ms, cur_loss, ppl))\n  mlflow.log_metric(log_prefix + '_milliseconds/batch', ms, step=batch)\n  mlflow.log_metric(log_prefix + '_loss', cur_loss, step=batch)\n  mlflow.log_metric(log_prefix + '_ppl', ppl, step=batch)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["####Define `train` and `evaluate` methods\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#run-the-model for details.\n\nThe `train` and `evaluate` methods are shared for both `baseline` and `ort` training. The boolean flag `accelerate_using_ort` is used for conditional execution needed for each approach."],"metadata":{}},{"cell_type":"code","source":["import time\ndef train(epoch, accelerate_using_ort):\n    if not accelerate_using_ort:\n      model.train() # Turn on the train mode\n      \n    total_loss = 0.\n    start_time = time.time()\n\n    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n        data, targets = get_batch(train_data, i)\n        \n        if not accelerate_using_ort:\n          optimizer.zero_grad()\n          output = model(data)\n          loss = calculate_loss(output, targets)\n          loss.backward()\n          torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n          optimizer.step()\n          current_learning_rate = learning_rate\n        else:\n          loss, output = trainer.train_step(data, targets)\n          current_learning_rate = learning_rate\n\n        total_loss += loss.item()\n        log_interval = 50\n\n        if batch % log_interval == 0 and batch > 0:\n            cur_loss = total_loss / log_interval\n            elapsed = time.time() - start_time\n\n            log_metrics(epoch, \n                        batch, \n                        len(train_data),\n                        bptt, \n                        current_learning_rate,\n                        elapsed,\n                        log_interval,\n                        cur_loss,\n                        metric_prefix)\n            \n            total_loss = 0\n            start_time = time.time()           "],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["def evaluate(eval_model, data_source, accelerate_using_ort=False):\n    if not accelerate_using_ort:\n      eval_model.eval() # Turn on the evaluation mode\n      \n    total_loss = 0.\n    ntokens = len(TEXT.vocab.stoi)\n    with torch.no_grad():\n        for i in range(0, data_source.size(0) - 1, bptt):\n            data, targets = get_batch(data_source, i)\n            if not accelerate_using_ort:\n              output = eval_model(data)\n              output_flat = output.view(-1, ntokens)\n              total_loss += len(data) * criterion(output_flat, targets).item()\n            else:\n              loss, outputs = trainer.eval_step(data, targets)\n              total_loss += len(data) * loss.item()              \n            \n    return total_loss / (len(data_source) - 1)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["####Define `train_model` method to execute the train loop\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#run-the-model for details.\n\nThis method is shared for both `baseline` and `ort` training. The boolean parameter `accelerate_using_ort` is used for conditional execution needed for each approach when calling `train` and `evaluate` methods."],"metadata":{}},{"cell_type":"code","source":["def train_model(accelerate_using_ort=False):\n  best_val_loss = float(\"inf\")\n  epochs = 3 # The number of epochs\n  best_model = None\n\n  for epoch in range(1, epochs + 1):\n      epoch_start_time = time.time()\n      \n      train(epoch, accelerate_using_ort)\n        \n      val_loss = evaluate(model, val_data, accelerate_using_ort)\n      print('-' * 89)\n      print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | valid ppl {:8.2f}'\n            .format(epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss)))\n      print('-' * 89)\n\n      if val_loss < best_val_loss:\n          best_val_loss = val_loss\n          best_model = model\n  return best_model"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["####Train model `without ONNX Runtime acceleration`"],"metadata":{}},{"cell_type":"code","source":["baseline_model = train_model(accelerate_using_ort=False)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["####Evaluate the model with the test dataset\nThe `baseline` model trained without ONNX Runtime accelerated is evaluated in the step below.\n\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#evaluate-the-model-with-the-test-dataset for details."],"metadata":{}},{"cell_type":"code","source":["test_loss = evaluate(baseline_model, test_data)\nprint('=' * 89)\nprint('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\nprint('=' * 89)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["###Accelerated model training"],"metadata":{}},{"cell_type":"markdown","source":["####Initiate an instance of the model for training `with ONNX Runtime acceleration` (referred to as `ort` training in this notebook)\n\nTo start with a new model from scratch, the step below instantiates a model with the additional code needed for accelerated training using ONNX Runtime. This step is necessary to make sure the model created in the section above titled `Initiate an instance of the model for training without ONNX Runtime acceleration (referred to as baseline training in this notebook)` is not reused in the steps below. Otherwise, loss and ppl metrics will not be comparable between the `baseline` and `ort` model training approaches shown in this notebook.\n\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#initiate-an-instance for details.\nRefer to https://github.com/microsoft/onnxruntime-training-examples/tree/master/getting-started for details on changes needed to accelerate a PyTorch implementation using ONNX Runtime (ORT)."],"metadata":{}},{"cell_type":"code","source":["from onnxruntime.training import ORTTrainer, optim\n\nmodel_description = {'inputs':  [('src', ['bptt', 'batch_size']),\n                                 ('label', ['bptt_x_batch_size'])],\n                     'outputs': [('loss', [], True),\n                                 ('output', ['bptt', 'batch_size', ntokens])]}\n\nmodel = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\ncriterion = nn.CrossEntropyLoss()\nlearning_rate = lr\noptimizer_config = optim.AdamConfig(lr=learning_rate)\n\ntrainer = ORTTrainer(model,               # model\n                     model_description,   # model description\n                     optimizer_config,    # optimizer configuration\n                     calculate_loss)      # loss function\nmetric_prefix = 'onnxruntime'"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["####Train model `with ONNX Runtime acceleration`"],"metadata":{}},{"cell_type":"code","source":["ort_model = train_model(accelerate_using_ort=True)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["####Evaluate the model with the test dataset\nThe `ort` model trained without ONNX Runtime accelerated is evaluated in the step below\n\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#evaluate-the-model-with-the-test-dataset for details."],"metadata":{}},{"cell_type":"code","source":["test_loss = evaluate(ort_model, test_data, accelerate_using_ort=True)\nprint('=' * 89)\nprint('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\nprint('=' * 89)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["###Summary\nThis notebook shows how to accelerate training of a `Transformer` model using `ONNX Runtime`. The optimizations on compute and memory utilization in `ONNX Runtime` **reduced the training time** of a simple sequence-to-sequence model by **~40%** (in each epoch and total training time) without any change in hyper parameters and without impacting training and test metrics on loss and ppl."],"metadata":{}}],"metadata":{"name":"ATTv2","notebookId":2937200619401804},"nbformat":4,"nbformat_minor":0}
