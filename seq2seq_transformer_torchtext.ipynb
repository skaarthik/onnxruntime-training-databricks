{"cells":[{"cell_type":"markdown","source":["## Accelerated Training of Sequence-to-Sequence Transformer Model\nThis notebook demonstrates using [ONNX Runtime](https://cloudblogs.microsoft.com/opensource/2020/05/19/announcing-support-for-accelerated-training-with-onnx-runtime/) to accelerate the training of a simple sequence-to-sequence model. It uses a slightly modified version of the implementation available at [PyTorch tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) on [nn.Transformer](https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer) module. This example uses [Adam](https://arxiv.org/abs/1412.6980) optimization instead of [SGD](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) that is used in the original implementation. This notebook is available at [https://github.com/skaarthik/onnxruntime-training-databricks](https://github.com/skaarthik/onnxruntime-training-databricks). Recipes on using ONNX Runtime to accelerate pretraining and finetuning of BERT and GPT-2 models are available at [https://github.com/microsoft/onnxruntime-training-examples](https://github.com/microsoft/onnxruntime-training-examples)."],"metadata":{}},{"cell_type":"markdown","source":["####Prerequisites for the demo\nDatabricks cluster:\n* Databricks cluster with `7.3 LTS ML` runtime\n* Node with `V100` GPU (like, `Standard_NC6s_v3` node in Azure)\n\nCustom containers are not supported on ML Runtime in Databricks. This demo needs ML Runtime and GPUs. As an alternative to packaging dependencies in a customer container, use the commands below to prepare the Conda environment in Databricks that will be used for this notebook:\n* pip install `torchtext`\n* pip install `onnx`\n* pip install `cerberus`\n* pip install `https://onnxtraining.blob.core.windows.net/ort-databricks-demo/onnxruntime_gpu-1.5.1-cp37-cp37m-linux_x86_64.whl`"],"metadata":{}},{"cell_type":"markdown","source":["####Define the model\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model for details."],"metadata":{}},{"cell_type":"code","source":["import math\nimport torch\nimport torch.nn as nn\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\nclass TransformerModel(nn.Module):\n\n    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n        super(TransformerModel, self).__init__()\n        self.model_type = 'Transformer'\n        self.src_mask = None\n        self.pos_encoder = PositionalEncoding(ninp, dropout)\n        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        self.ninp = ninp\n        self.decoder = nn.Linear(ninp, ntoken)\n\n        self.init_weights()\n\n    def _generate_square_subsequent_mask(self, src):\n        sz = src.size(0)\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src):\n        if self.src_mask == None or self.src_mask.size(0) != src.size(0):\n            device = src.device\n            mask = self._generate_square_subsequent_mask(src).to(device)\n            self.src_mask = mask\n\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, self.src_mask)\n        output = self.decoder(output)\n        return output"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["####Load and batch data\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#load-and-batch-data for details."],"metadata":{}},{"cell_type":"code","source":["import torchtext\nfrom torchtext.data.utils import get_tokenizer\nTEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n                            init_token='<sos>',\n                            eos_token='<eos>',\n                            lower=True)\ntrain_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\nTEXT.build_vocab(train_txt)\ndevice = torch.device(\"cuda\")"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/local_disk0/.ephemeral_nfs/envs/pythonEnv-602ff9fa-7201-4e60-9e7f-b37e285a654b/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n  warnings.warn(&#39;{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.&#39;.format(self.__class__.__name__), UserWarning)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-602ff9fa-7201-4e60-9e7f-b37e285a654b/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n  warnings.warn(&#39;Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.&#39;, UserWarning)\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["####Functions to generate input and target sequence\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#functions-to-generate-input-and-target-sequence for details."],"metadata":{}},{"cell_type":"code","source":["bptt = 35\ndef get_batch(source, i):\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].view(-1)\n    return data, target\n  \ndef batchify(data, bsz):\n    data = TEXT.numericalize([data.examples[0].text])\n    # Divide the dataset into bsz parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    data = data.view(bsz, -1).t().contiguous()\n    return data.to(device)\n\nbatch_size = 20\neval_batch_size = 10\ntrain_data = batchify(train_txt, batch_size)\nval_data = batchify(val_txt, eval_batch_size)\ntest_data = batchify(test_txt, eval_batch_size)  "],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["####Initialize variables needed for model creation\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#initiate-an-instance for details."],"metadata":{}},{"cell_type":"code","source":["ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\nemsize = 200 # embedding dimension\nnhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\nnlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\nnhead = 2 # the number of heads in the multiheadattention models\ndropout = 0.2 # the dropout value\nlr = 0.001 # learning rate\n\ndef calculate_loss(output, targets):\n    output = output.view(-1, len(TEXT.vocab.stoi))\n    return criterion(output, targets) "],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["####Initiate an instance of the model for training `without ONNX Runtime acceleration` (referred to as `baseline` training in this notebook)\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#initiate-an-instance for details.\n\nAfter the next few steps below that are required for `baseline` training, this notebook will cover the steps required for accelerated training using ONNX Runtime (and that is referred to as `ort` training)."],"metadata":{}},{"cell_type":"code","source":["model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\ncriterion = nn.CrossEntropyLoss()\nlearning_rate = lr\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nmetric_prefix = 'baseline'  "],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["####Define function to use mlflow logging"],"metadata":{}},{"cell_type":"code","source":["import mlflow\ndef log_metrics(epoch, batch, train_data_len, bptt, lr, elapsed, log_interval, cur_loss, log_prefix):\n  ms = elapsed * 1000 / log_interval\n  ppl = math.exp(cur_loss)\n  print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.3f} | ms/batch {:5.2f} | loss {:5.2f} | ppl {:8.2f}'.\n        format(epoch, batch, train_data_len // bptt, lr, ms, cur_loss, ppl))\n  mlflow.log_metric(log_prefix + '_milliseconds/batch', ms, step=batch)\n  mlflow.log_metric(log_prefix + '_loss', cur_loss, step=batch)\n  mlflow.log_metric(log_prefix + '_ppl', ppl, step=batch)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["####Define `train` and `evaluate` methods\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#run-the-model for details.\n\nThe `train` and `evaluate` methods are shared for both `baseline` and `ort` training. The boolean flag `accelerate_using_ort` is used for conditional execution needed for each approach."],"metadata":{}},{"cell_type":"code","source":["import time\ndef train(epoch, accelerate_using_ort):\n    if not accelerate_using_ort:\n      model.train() # Turn on the train mode\n      \n    total_loss = 0.\n    start_time = time.time()\n\n    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n        data, targets = get_batch(train_data, i)\n        \n        if not accelerate_using_ort:\n          optimizer.zero_grad()\n          output = model(data)\n          loss = calculate_loss(output, targets)\n          loss.backward()\n          torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n          optimizer.step()\n          current_learning_rate = learning_rate\n        else:\n          loss, output = trainer.train_step(data, targets)\n          current_learning_rate = learning_rate\n\n        total_loss += loss.item()\n        log_interval = 50\n\n        if batch % log_interval == 0 and batch > 0:\n            cur_loss = total_loss / log_interval\n            elapsed = time.time() - start_time\n\n            log_metrics(epoch, \n                        batch, \n                        len(train_data),\n                        bptt, \n                        current_learning_rate,\n                        elapsed,\n                        log_interval,\n                        cur_loss,\n                        metric_prefix)\n            \n            total_loss = 0\n            start_time = time.time()           "],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["def evaluate(eval_model, data_source, accelerate_using_ort=False):\n    if not accelerate_using_ort:\n      eval_model.eval() # Turn on the evaluation mode\n      \n    total_loss = 0.\n    ntokens = len(TEXT.vocab.stoi)\n    with torch.no_grad():\n        for i in range(0, data_source.size(0) - 1, bptt):\n            data, targets = get_batch(data_source, i)\n            if not accelerate_using_ort:\n              output = eval_model(data)\n              output_flat = output.view(-1, ntokens)\n              total_loss += len(data) * criterion(output_flat, targets).item()\n            else:\n              loss, outputs = trainer.eval_step(data, targets)\n              total_loss += len(data) * loss.item()              \n            \n    return total_loss / (len(data_source) - 1)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["####Define `train_model` method to execute the train loop\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#run-the-model for details.\n\nThis method is shared for both `baseline` and `ort` training. The boolean parameter `accelerate_using_ort` is used for conditional execution needed for each approach when calling `train` and `evaluate` methods."],"metadata":{}},{"cell_type":"code","source":["def train_model(accelerate_using_ort=False):\n  best_val_loss = float(\"inf\")\n  epochs = 3 # The number of epochs\n  best_model = None\n\n  for epoch in range(1, epochs + 1):\n      epoch_start_time = time.time()\n      \n      train(epoch, accelerate_using_ort)\n        \n      val_loss = evaluate(model, val_data, accelerate_using_ort)\n      print('-' * 89)\n      print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | valid ppl {:8.2f}'\n            .format(epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss)))\n      print('-' * 89)\n\n      if val_loss < best_val_loss:\n          best_val_loss = val_loss\n          best_model = model\n  return best_model"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["####Train model `without ONNX Runtime acceleration`"],"metadata":{}},{"cell_type":"code","source":["baseline_model = train_model(accelerate_using_ort=False)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">| epoch   1 |    50/ 2981 batches | lr 0.001 | ms/batch 17.12 | loss  7.76 | ppl  2349.91\n epoch   1 |   100/ 2981 batches | lr 0.001 | ms/batch 12.28 | loss  6.68 | ppl   795.30\n epoch   1 |   150/ 2981 batches | lr 0.001 | ms/batch 12.37 | loss  6.49 | ppl   657.16\n epoch   1 |   200/ 2981 batches | lr 0.001 | ms/batch 12.32 | loss  6.34 | ppl   566.82\n epoch   1 |   250/ 2981 batches | lr 0.001 | ms/batch 12.25 | loss  6.24 | ppl   513.52\n epoch   1 |   300/ 2981 batches | lr 0.001 | ms/batch 12.32 | loss  6.22 | ppl   503.49\n epoch   1 |   350/ 2981 batches | lr 0.001 | ms/batch 12.23 | loss  6.09 | ppl   440.51\n epoch   1 |   400/ 2981 batches | lr 0.001 | ms/batch 12.39 | loss  6.07 | ppl   433.87\n epoch   1 |   450/ 2981 batches | lr 0.001 | ms/batch 12.18 | loss  5.94 | ppl   381.13\n epoch   1 |   500/ 2981 batches | lr 0.001 | ms/batch 12.29 | loss  6.01 | ppl   405.89\n epoch   1 |   550/ 2981 batches | lr 0.001 | ms/batch 12.48 | loss  5.88 | ppl   358.32\n epoch   1 |   600/ 2981 batches | lr 0.001 | ms/batch 12.32 | loss  5.79 | ppl   328.40\n epoch   1 |   650/ 2981 batches | lr 0.001 | ms/batch 12.36 | loss  5.92 | ppl   373.04\n epoch   1 |   700/ 2981 batches | lr 0.001 | ms/batch 12.38 | loss  5.84 | ppl   345.35\n epoch   1 |   750/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  5.79 | ppl   326.18\n epoch   1 |   800/ 2981 batches | lr 0.001 | ms/batch 12.33 | loss  5.87 | ppl   355.86\n epoch   1 |   850/ 2981 batches | lr 0.001 | ms/batch 12.41 | loss  5.77 | ppl   320.91\n epoch   1 |   900/ 2981 batches | lr 0.001 | ms/batch 12.43 | loss  5.74 | ppl   310.26\n epoch   1 |   950/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  5.78 | ppl   325.19\n epoch   1 |  1000/ 2981 batches | lr 0.001 | ms/batch 12.34 | loss  5.71 | ppl   301.21\n epoch   1 |  1050/ 2981 batches | lr 0.001 | ms/batch 12.33 | loss  5.78 | ppl   325.27\n epoch   1 |  1100/ 2981 batches | lr 0.001 | ms/batch 12.19 | loss  5.69 | ppl   296.79\n epoch   1 |  1150/ 2981 batches | lr 0.001 | ms/batch 12.32 | loss  5.69 | ppl   297.25\n epoch   1 |  1200/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  5.80 | ppl   328.95\n epoch   1 |  1250/ 2981 batches | lr 0.001 | ms/batch 12.40 | loss  5.74 | ppl   311.23\n epoch   1 |  1300/ 2981 batches | lr 0.001 | ms/batch 12.31 | loss  5.80 | ppl   331.76\n epoch   1 |  1350/ 2981 batches | lr 0.001 | ms/batch 12.33 | loss  5.68 | ppl   292.42\n epoch   1 |  1400/ 2981 batches | lr 0.001 | ms/batch 12.32 | loss  5.66 | ppl   286.18\n epoch   1 |  1450/ 2981 batches | lr 0.001 | ms/batch 12.40 | loss  5.75 | ppl   313.90\n epoch   1 |  1500/ 2981 batches | lr 0.001 | ms/batch 12.34 | loss  5.78 | ppl   323.19\n epoch   1 |  1550/ 2981 batches | lr 0.001 | ms/batch 12.42 | loss  5.72 | ppl   303.45\n epoch   1 |  1600/ 2981 batches | lr 0.001 | ms/batch 12.38 | loss  5.65 | ppl   283.13\n epoch   1 |  1650/ 2981 batches | lr 0.001 | ms/batch 12.48 | loss  5.60 | ppl   269.33\n epoch   1 |  1700/ 2981 batches | lr 0.001 | ms/batch 12.38 | loss  5.61 | ppl   273.78\n epoch   1 |  1750/ 2981 batches | lr 0.001 | ms/batch 12.56 | loss  5.60 | ppl   270.04\n epoch   1 |  1800/ 2981 batches | lr 0.001 | ms/batch 12.57 | loss  5.63 | ppl   277.81\n epoch   1 |  1850/ 2981 batches | lr 0.001 | ms/batch 12.32 | loss  5.68 | ppl   292.01\n epoch   1 |  1900/ 2981 batches | lr 0.001 | ms/batch 12.29 | loss  5.62 | ppl   274.70\n epoch   1 |  1950/ 2981 batches | lr 0.001 | ms/batch 12.62 | loss  5.58 | ppl   266.08\n epoch   1 |  2000/ 2981 batches | lr 0.001 | ms/batch 12.36 | loss  5.58 | ppl   265.75\n epoch   1 |  2050/ 2981 batches | lr 0.001 | ms/batch 12.41 | loss  5.58 | ppl   264.53\n epoch   1 |  2100/ 2981 batches | lr 0.001 | ms/batch 12.26 | loss  5.47 | ppl   237.29\n epoch   1 |  2150/ 2981 batches | lr 0.001 | ms/batch 12.42 | loss  5.42 | ppl   225.65\n epoch   1 |  2200/ 2981 batches | lr 0.001 | ms/batch 12.43 | loss  5.47 | ppl   238.57\n epoch   1 |  2250/ 2981 batches | lr 0.001 | ms/batch 12.32 | loss  5.48 | ppl   239.53\n epoch   1 |  2300/ 2981 batches | lr 0.001 | ms/batch 12.34 | loss  5.58 | ppl   263.93\n epoch   1 |  2350/ 2981 batches | lr 0.001 | ms/batch 12.33 | loss  5.63 | ppl   278.29\n epoch   1 |  2400/ 2981 batches | lr 0.001 | ms/batch 12.28 | loss  5.56 | ppl   259.76\n epoch   1 |  2450/ 2981 batches | lr 0.001 | ms/batch 12.37 | loss  5.50 | ppl   244.73\n epoch   1 |  2500/ 2981 batches | lr 0.001 | ms/batch 12.27 | loss  5.53 | ppl   251.47\n epoch   1 |  2550/ 2981 batches | lr 0.001 | ms/batch 12.29 | loss  5.54 | ppl   254.57\n epoch   1 |  2600/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  5.60 | ppl   271.02\n epoch   1 |  2650/ 2981 batches | lr 0.001 | ms/batch 12.36 | loss  5.45 | ppl   233.63\n epoch   1 |  2700/ 2981 batches | lr 0.001 | ms/batch 12.38 | loss  5.37 | ppl   215.54\n epoch   1 |  2750/ 2981 batches | lr 0.001 | ms/batch 12.34 | loss  5.49 | ppl   241.67\n epoch   1 |  2800/ 2981 batches | lr 0.001 | ms/batch 12.41 | loss  5.46 | ppl   234.43\n epoch   1 |  2850/ 2981 batches | lr 0.001 | ms/batch 12.25 | loss  5.46 | ppl   235.20\n epoch   1 |  2900/ 2981 batches | lr 0.001 | ms/batch 12.32 | loss  5.36 | ppl   213.26\n epoch   1 |  2950/ 2981 batches | lr 0.001 | ms/batch 12.46 | loss  5.29 | ppl   199.21\n-----------------------------------------------------------------------------------------\n end of epoch   1 | time: 61.13s | valid loss  5.38 | valid ppl   218.07\n-----------------------------------------------------------------------------------------\n epoch   2 |    50/ 2981 batches | lr 0.001 | ms/batch 13.10 | loss  5.38 | ppl   216.55\n epoch   2 |   100/ 2981 batches | lr 0.001 | ms/batch 12.44 | loss  5.20 | ppl   181.21\n epoch   2 |   150/ 2981 batches | lr 0.001 | ms/batch 12.37 | loss  5.21 | ppl   182.74\n epoch   2 |   200/ 2981 batches | lr 0.001 | ms/batch 12.39 | loss  5.19 | ppl   179.82\n epoch   2 |   250/ 2981 batches | lr 0.001 | ms/batch 12.37 | loss  5.20 | ppl   181.29\n epoch   2 |   300/ 2981 batches | lr 0.001 | ms/batch 12.38 | loss  5.27 | ppl   193.95\n epoch   2 |   350/ 2981 batches | lr 0.001 | ms/batch 12.43 | loss  5.20 | ppl   181.66\n epoch   2 |   400/ 2981 batches | lr 0.001 | ms/batch 12.36 | loss  5.20 | ppl   181.58\n epoch   2 |   450/ 2981 batches | lr 0.001 | ms/batch 12.38 | loss  5.02 | ppl   151.79\n epoch   2 |   500/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  5.14 | ppl   171.36\n epoch   2 |   550/ 2981 batches | lr 0.001 | ms/batch 12.60 | loss  5.01 | ppl   149.98\n epoch   2 |   600/ 2981 batches | lr 0.001 | ms/batch 12.36 | loss  4.97 | ppl   143.97\n epoch   2 |   650/ 2981 batches | lr 0.001 | ms/batch 12.34 | loss  5.09 | ppl   162.16\n epoch   2 |   700/ 2981 batches | lr 0.001 | ms/batch 12.27 | loss  5.06 | ppl   157.44\n epoch   2 |   750/ 2981 batches | lr 0.001 | ms/batch 12.52 | loss  5.06 | ppl   156.87\n epoch   2 |   800/ 2981 batches | lr 0.001 | ms/batch 12.33 | loss  5.15 | ppl   173.04\n epoch   2 |   850/ 2981 batches | lr 0.001 | ms/batch 12.33 | loss  5.08 | ppl   160.48\n epoch   2 |   900/ 2981 batches | lr 0.001 | ms/batch 12.60 | loss  5.04 | ppl   153.86\n epoch   2 |   950/ 2981 batches | lr 0.001 | ms/batch 12.32 | loss  5.08 | ppl   160.26\n epoch   2 |  1000/ 2981 batches | lr 0.001 | ms/batch 12.34 | loss  5.01 | ppl   149.16\n epoch   2 |  1050/ 2981 batches | lr 0.001 | ms/batch 12.18 | loss  5.08 | ppl   161.45\n epoch   2 |  1100/ 2981 batches | lr 0.001 | ms/batch 12.41 | loss  5.01 | ppl   149.22\n epoch   2 |  1150/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  5.02 | ppl   150.81\n epoch   2 |  1200/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  5.13 | ppl   169.34\n epoch   2 |  1250/ 2981 batches | lr 0.001 | ms/batch 12.27 | loss  5.13 | ppl   168.89\n epoch   2 |  1300/ 2981 batches | lr 0.001 | ms/batch 12.43 | loss  5.17 | ppl   175.19\n epoch   2 |  1350/ 2981 batches | lr 0.001 | ms/batch 12.26 | loss  5.05 | ppl   156.50\n epoch   2 |  1400/ 2981 batches | lr 0.001 | ms/batch 12.36 | loss  5.02 | ppl   150.91\n epoch   2 |  1450/ 2981 batches | lr 0.001 | ms/batch 12.30 | loss  5.10 | ppl   163.88\n epoch   2 |  1500/ 2981 batches | lr 0.001 | ms/batch 12.34 | loss  5.18 | ppl   177.61\n epoch   2 |  1550/ 2981 batches | lr 0.001 | ms/batch 12.37 | loss  5.14 | ppl   171.24\n epoch   2 |  1600/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  5.07 | ppl   159.62\n epoch   2 |  1650/ 2981 batches | lr 0.001 | ms/batch 12.22 | loss  4.99 | ppl   147.53\n epoch   2 |  1700/ 2981 batches | lr 0.001 | ms/batch 12.30 | loss  5.05 | ppl   155.55\n epoch   2 |  1750/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  5.04 | ppl   154.82\n epoch   2 |  1800/ 2981 batches | lr 0.001 | ms/batch 12.40 | loss  5.08 | ppl   160.01\n epoch   2 |  1850/ 2981 batches | lr 0.001 | ms/batch 12.34 | loss  5.13 | ppl   168.50\n epoch   2 |  1900/ 2981 batches | lr 0.001 | ms/batch 12.33 | loss  5.07 | ppl   159.15\n epoch   2 |  1950/ 2981 batches | lr 0.001 | ms/batch 12.33 | loss  5.04 | ppl   153.79\n epoch   2 |  2000/ 2981 batches | lr 0.001 | ms/batch 12.36 | loss  5.05 | ppl   156.08\n epoch   2 |  2050/ 2981 batches | lr 0.001 | ms/batch 12.71 | loss  5.02 | ppl   151.34\n epoch   2 |  2100/ 2981 batches | lr 0.001 | ms/batch 12.59 | loss  4.91 | ppl   135.80\n epoch   2 |  2150/ 2981 batches | lr 0.001 | ms/batch 12.53 | loss  4.88 | ppl   131.14\n epoch   2 |  2200/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  4.92 | ppl   136.83\n epoch   2 |  2250/ 2981 batches | lr 0.001 | ms/batch 12.19 | loss  4.92 | ppl   136.35\n epoch   2 |  2300/ 2981 batches | lr 0.001 | ms/batch 12.21 | loss  5.03 | ppl   152.33\n epoch   2 |  2350/ 2981 batches | lr 0.001 | ms/batch 12.22 | loss  5.09 | ppl   161.77\n epoch   2 |  2400/ 2981 batches | lr 0.001 | ms/batch 12.44 | loss  5.02 | ppl   151.45\n epoch   2 |  2450/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  4.97 | ppl   143.39\n epoch   2 |  2500/ 2981 batches | lr 0.001 | ms/batch 12.42 | loss  5.00 | ppl   147.85\n epoch   2 |  2550/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  5.04 | ppl   153.92\n epoch   2 |  2600/ 2981 batches | lr 0.001 | ms/batch 12.41 | loss  5.10 | ppl   163.27\n epoch   2 |  2650/ 2981 batches | lr 0.001 | ms/batch 12.34 | loss  4.98 | ppl   145.89\n epoch   2 |  2700/ 2981 batches | lr 0.001 | ms/batch 12.36 | loss  4.91 | ppl   135.86\n epoch   2 |  2750/ 2981 batches | lr 0.001 | ms/batch 12.34 | loss  4.97 | ppl   143.87\n epoch   2 |  2800/ 2981 batches | lr 0.001 | ms/batch 12.37 | loss  4.96 | ppl   142.89\n epoch   2 |  2850/ 2981 batches | lr 0.001 | ms/batch 12.32 | loss  4.97 | ppl   144.52\n epoch   2 |  2900/ 2981 batches | lr 0.001 | ms/batch 12.64 | loss  4.89 | ppl   133.24\n epoch   2 |  2950/ 2981 batches | lr 0.001 | ms/batch 12.53 | loss  4.83 | ppl   125.82\n-----------------------------------------------------------------------------------------\n end of epoch   2 | time: 58.43s | valid loss  5.26 | valid ppl   192.64\n-----------------------------------------------------------------------------------------\n epoch   3 |    50/ 2981 batches | lr 0.001 | ms/batch 12.86 | loss  4.98 | ppl   145.28\n epoch   3 |   100/ 2981 batches | lr 0.001 | ms/batch 12.77 | loss  4.81 | ppl   123.09\n epoch   3 |   150/ 2981 batches | lr 0.001 | ms/batch 12.38 | loss  4.83 | ppl   124.79\n epoch   3 |   200/ 2981 batches | lr 0.001 | ms/batch 12.36 | loss  4.84 | ppl   126.38\n epoch   3 |   250/ 2981 batches | lr 0.001 | ms/batch 12.33 | loss  4.86 | ppl   128.68\n epoch   3 |   300/ 2981 batches | lr 0.001 | ms/batch 12.50 | loss  4.91 | ppl   135.90\n epoch   3 |   350/ 2981 batches | lr 0.001 | ms/batch 12.39 | loss  4.85 | ppl   128.38\n epoch   3 |   400/ 2981 batches | lr 0.001 | ms/batch 12.44 | loss  4.86 | ppl   129.40\n epoch   3 |   450/ 2981 batches | lr 0.001 | ms/batch 12.30 | loss  4.67 | ppl   106.49\n epoch   3 |   500/ 2981 batches | lr 0.001 | ms/batch 12.31 | loss  4.80 | ppl   121.29\n epoch   3 |   550/ 2981 batches | lr 0.001 | ms/batch 12.30 | loss  4.68 | ppl   107.33\n epoch   3 |   600/ 2981 batches | lr 0.001 | ms/batch 12.18 | loss  4.65 | ppl   105.01\n epoch   3 |   650/ 2981 batches | lr 0.001 | ms/batch 12.28 | loss  4.74 | ppl   114.19\n epoch   3 |   700/ 2981 batches | lr 0.001 | ms/batch 12.36 | loss  4.72 | ppl   112.38\n epoch   3 |   750/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  4.74 | ppl   115.00\n epoch   3 |   800/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  4.84 | ppl   126.93\n epoch   3 |   850/ 2981 batches | lr 0.001 | ms/batch 12.26 | loss  4.77 | ppl   117.70\n epoch   3 |   900/ 2981 batches | lr 0.001 | ms/batch 12.39 | loss  4.73 | ppl   113.21\n epoch   3 |   950/ 2981 batches | lr 0.001 | ms/batch 12.36 | loss  4.77 | ppl   118.07\n epoch   3 |  1000/ 2981 batches | lr 0.001 | ms/batch 12.38 | loss  4.70 | ppl   109.46\n epoch   3 |  1050/ 2981 batches | lr 0.001 | ms/batch 12.30 | loss  4.78 | ppl   118.79\n epoch   3 |  1100/ 2981 batches | lr 0.001 | ms/batch 12.47 | loss  4.70 | ppl   110.06\n epoch   3 |  1150/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  4.72 | ppl   112.65\n epoch   3 |  1200/ 2981 batches | lr 0.001 | ms/batch 12.39 | loss  4.84 | ppl   126.89\n epoch   3 |  1250/ 2981 batches | lr 0.001 | ms/batch 12.55 | loss  4.84 | ppl   126.77\n epoch   3 |  1300/ 2981 batches | lr 0.001 | ms/batch 12.31 | loss  4.87 | ppl   130.01\n epoch   3 |  1350/ 2981 batches | lr 0.001 | ms/batch 12.41 | loss  4.76 | ppl   116.32\n epoch   3 |  1400/ 2981 batches | lr 0.001 | ms/batch 12.54 | loss  4.73 | ppl   113.48\n epoch   3 |  1450/ 2981 batches | lr 0.001 | ms/batch 12.51 | loss  4.80 | ppl   121.34\n epoch   3 |  1500/ 2981 batches | lr 0.001 | ms/batch 12.41 | loss  4.89 | ppl   132.54\n epoch   3 |  1550/ 2981 batches | lr 0.001 | ms/batch 12.34 | loss  4.86 | ppl   129.04\n epoch   3 |  1600/ 2981 batches | lr 0.001 | ms/batch 12.55 | loss  4.81 | ppl   122.75\n epoch   3 |  1650/ 2981 batches | lr 0.001 | ms/batch 12.22 | loss  4.71 | ppl   111.32\n epoch   3 |  1700/ 2981 batches | lr 0.001 | ms/batch 12.58 | loss  4.77 | ppl   118.16\n epoch   3 |  1750/ 2981 batches | lr 0.001 | ms/batch 12.38 | loss  4.77 | ppl   118.26\n epoch   3 |  1800/ 2981 batches | lr 0.001 | ms/batch 12.33 | loss  4.81 | ppl   123.02\n epoch   3 |  1850/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  4.85 | ppl   127.26\n epoch   3 |  1900/ 2981 batches | lr 0.001 | ms/batch 12.36 | loss  4.80 | ppl   122.02\n epoch   3 |  1950/ 2981 batches | lr 0.001 | ms/batch 12.38 | loss  4.77 | ppl   117.54\n epoch   3 |  2000/ 2981 batches | lr 0.001 | ms/batch 12.36 | loss  4.78 | ppl   118.66\n epoch   3 |  2050/ 2981 batches | lr 0.001 | ms/batch 12.34 | loss  4.75 | ppl   116.06\n epoch   3 |  2100/ 2981 batches | lr 0.001 | ms/batch 12.33 | loss  4.65 | ppl   104.17\n epoch   3 |  2150/ 2981 batches | lr 0.001 | ms/batch 12.38 | loss  4.61 | ppl   100.39\n epoch   3 |  2200/ 2981 batches | lr 0.001 | ms/batch 12.48 | loss  4.64 | ppl   103.58\n epoch   3 |  2250/ 2981 batches | lr 0.001 | ms/batch 12.71 | loss  4.64 | ppl   103.43\n epoch   3 |  2300/ 2981 batches | lr 0.001 | ms/batch 12.38 | loss  4.74 | ppl   114.72\n epoch   3 |  2350/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  4.82 | ppl   124.17\n epoch   3 |  2400/ 2981 batches | lr 0.001 | ms/batch 12.39 | loss  4.74 | ppl   114.18\n epoch   3 |  2450/ 2981 batches | lr 0.001 | ms/batch 12.38 | loss  4.71 | ppl   111.07\n epoch   3 |  2500/ 2981 batches | lr 0.001 | ms/batch 12.43 | loss  4.74 | ppl   114.43\n epoch   3 |  2550/ 2981 batches | lr 0.001 | ms/batch 12.72 | loss  4.76 | ppl   117.13\n epoch   3 |  2600/ 2981 batches | lr 0.001 | ms/batch 12.31 | loss  4.84 | ppl   126.39\n epoch   3 |  2650/ 2981 batches | lr 0.001 | ms/batch 12.58 | loss  4.73 | ppl   113.17\n epoch   3 |  2700/ 2981 batches | lr 0.001 | ms/batch 12.41 | loss  4.67 | ppl   106.56\n epoch   3 |  2750/ 2981 batches | lr 0.001 | ms/batch 12.44 | loss  4.70 | ppl   110.01\n epoch   3 |  2800/ 2981 batches | lr 0.001 | ms/batch 12.53 | loss  4.70 | ppl   110.21\n epoch   3 |  2850/ 2981 batches | lr 0.001 | ms/batch 12.42 | loss  4.71 | ppl   111.34\n epoch   3 |  2900/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  4.65 | ppl   104.86\n epoch   3 |  2950/ 2981 batches | lr 0.001 | ms/batch 12.50 | loss  4.59 | ppl    98.69\n-----------------------------------------------------------------------------------------\n end of epoch   3 | time: 58.52s | valid loss  5.25 | valid ppl   191.00\n-----------------------------------------------------------------------------------------\n</div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["####Evaluate the model with the test dataset\nThe `baseline` model trained without ONNX Runtime accelerated is evaluated in the step below.\n\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#evaluate-the-model-with-the-test-dataset for details."],"metadata":{}},{"cell_type":"code","source":["test_loss = evaluate(baseline_model, test_data)\nprint('=' * 89)\nprint('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\nprint('=' * 89)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">=========================================================================================\n End of training | test loss  5.17 | test ppl   176.09\n=========================================================================================\n</div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["###Accelerated model training"],"metadata":{}},{"cell_type":"markdown","source":["####Initiate an instance of the model for training `with ONNX Runtime acceleration` (referred to as `ort` training in this notebook)\n\nTo start with a new model from scratch, the step below instantiates a model with the additional code needed for accelerated training using ONNX Runtime. This step is necessary to make sure the model created in the section above titled `Initiate an instance of the model for training without ONNX Runtime acceleration (referred to as baseline training in this notebook)` is not reused in the steps below. Otherwise, loss and ppl metrics will not be comparable between the `baseline` and `ort` model training approaches shown in this notebook.\n\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#initiate-an-instance for details.\nRefer to https://github.com/microsoft/onnxruntime-training-examples/tree/master/getting-started for details on changes needed to accelerate a PyTorch implementation using ONNX Runtime (ORT)."],"metadata":{}},{"cell_type":"code","source":["from onnxruntime.training import ORTTrainer, optim\n\nmodel_description = {'inputs':  [('src', ['bptt', 'batch_size']),\n                                 ('label', ['bptt_x_batch_size'])],\n                     'outputs': [('loss', [], True),\n                                 ('output', ['bptt', 'batch_size', ntokens])]}\n\nmodel = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\ncriterion = nn.CrossEntropyLoss()\nlearning_rate = lr\noptimizer_config = optim.AdamConfig(lr=learning_rate)\n\ntrainer = ORTTrainer(model,               # model\n                     model_description,   # model description\n                     optimizer_config,    # optimizer configuration\n                     calculate_loss)      # loss function\nmetric_prefix = 'onnxruntime'"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["####Train model `with ONNX Runtime acceleration`"],"metadata":{}},{"cell_type":"code","source":["ort_model = train_model(accelerate_using_ort=True)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/local_disk0/tmp/1603672713973-0/PythonShell.py:21: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can&#39;t record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n  from tempfile import NamedTemporaryFile\n epoch   1 |    50/ 2981 batches | lr 0.001 | ms/batch 50.18 | loss  7.74 | ppl  2303.05\n epoch   1 |   100/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  6.89 | ppl   979.17\n epoch   1 |   150/ 2981 batches | lr 0.001 | ms/batch  6.35 | loss  6.62 | ppl   751.29\n epoch   1 |   200/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  6.42 | ppl   613.98\n epoch   1 |   250/ 2981 batches | lr 0.001 | ms/batch  6.17 | loss  6.31 | ppl   550.52\n epoch   1 |   300/ 2981 batches | lr 0.001 | ms/batch  6.12 | loss  6.26 | ppl   521.70\n epoch   1 |   350/ 2981 batches | lr 0.001 | ms/batch  6.23 | loss  6.11 | ppl   451.02\n epoch   1 |   400/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  6.09 | ppl   439.70\n epoch   1 |   450/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  5.96 | ppl   386.18\n epoch   1 |   500/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  6.01 | ppl   409.28\n epoch   1 |   550/ 2981 batches | lr 0.001 | ms/batch  6.22 | loss  5.89 | ppl   362.34\n epoch   1 |   600/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.81 | ppl   333.74\n epoch   1 |   650/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.94 | ppl   380.70\n epoch   1 |   700/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.84 | ppl   344.15\n epoch   1 |   750/ 2981 batches | lr 0.001 | ms/batch  6.18 | loss  5.79 | ppl   327.36\n epoch   1 |   800/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.88 | ppl   357.87\n epoch   1 |   850/ 2981 batches | lr 0.001 | ms/batch  6.30 | loss  5.78 | ppl   323.53\n epoch   1 |   900/ 2981 batches | lr 0.001 | ms/batch  6.12 | loss  5.74 | ppl   310.39\n epoch   1 |   950/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.78 | ppl   324.06\n epoch   1 |  1000/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.71 | ppl   302.10\n epoch   1 |  1050/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.78 | ppl   325.37\n epoch   1 |  1100/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.69 | ppl   295.57\n epoch   1 |  1150/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  5.69 | ppl   296.96\n epoch   1 |  1200/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  5.79 | ppl   328.19\n epoch   1 |  1250/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.75 | ppl   312.83\n epoch   1 |  1300/ 2981 batches | lr 0.001 | ms/batch  6.12 | loss  5.81 | ppl   333.37\n epoch   1 |  1350/ 2981 batches | lr 0.001 | ms/batch  6.16 | loss  5.68 | ppl   293.89\n epoch   1 |  1400/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.66 | ppl   285.88\n epoch   1 |  1450/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  5.75 | ppl   313.82\n epoch   1 |  1500/ 2981 batches | lr 0.001 | ms/batch  6.18 | loss  5.78 | ppl   324.67\n epoch   1 |  1550/ 2981 batches | lr 0.001 | ms/batch  6.16 | loss  5.72 | ppl   303.43\n epoch   1 |  1600/ 2981 batches | lr 0.001 | ms/batch  6.17 | loss  5.65 | ppl   285.00\n epoch   1 |  1650/ 2981 batches | lr 0.001 | ms/batch  6.20 | loss  5.60 | ppl   271.69\n epoch   1 |  1700/ 2981 batches | lr 0.001 | ms/batch  6.32 | loss  5.61 | ppl   272.90\n epoch   1 |  1750/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  5.60 | ppl   271.40\n epoch   1 |  1800/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.63 | ppl   279.21\n epoch   1 |  1850/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.68 | ppl   293.17\n epoch   1 |  1900/ 2981 batches | lr 0.001 | ms/batch  6.19 | loss  5.61 | ppl   273.97\n epoch   1 |  1950/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.58 | ppl   264.22\n epoch   1 |  2000/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  5.59 | ppl   268.55\n epoch   1 |  2050/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.58 | ppl   265.30\n epoch   1 |  2100/ 2981 batches | lr 0.001 | ms/batch  6.17 | loss  5.47 | ppl   237.20\n epoch   1 |  2150/ 2981 batches | lr 0.001 | ms/batch  6.22 | loss  5.42 | ppl   225.07\n epoch   1 |  2200/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.47 | ppl   237.11\n epoch   1 |  2250/ 2981 batches | lr 0.001 | ms/batch  6.12 | loss  5.47 | ppl   237.64\n epoch   1 |  2300/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.57 | ppl   262.61\n epoch   1 |  2350/ 2981 batches | lr 0.001 | ms/batch  6.20 | loss  5.63 | ppl   279.71\n epoch   1 |  2400/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.56 | ppl   259.53\n epoch   1 |  2450/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.50 | ppl   244.26\n epoch   1 |  2500/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.52 | ppl   249.96\n epoch   1 |  2550/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  5.54 | ppl   253.91\n epoch   1 |  2600/ 2981 batches | lr 0.001 | ms/batch  6.42 | loss  5.60 | ppl   269.20\n epoch   1 |  2650/ 2981 batches | lr 0.001 | ms/batch  6.20 | loss  5.45 | ppl   233.23\n epoch   1 |  2700/ 2981 batches | lr 0.001 | ms/batch  6.17 | loss  5.38 | ppl   217.95\n epoch   1 |  2750/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.49 | ppl   243.24\n epoch   1 |  2800/ 2981 batches | lr 0.001 | ms/batch  6.21 | loss  5.46 | ppl   234.80\n epoch   1 |  2850/ 2981 batches | lr 0.001 | ms/batch  6.16 | loss  5.46 | ppl   236.08\n epoch   1 |  2900/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.35 | ppl   210.77\n epoch   1 |  2950/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.30 | ppl   199.72\n-----------------------------------------------------------------------------------------\n end of epoch   1 | time: 40.95s | valid loss  5.37 | valid ppl   215.90\n-----------------------------------------------------------------------------------------\n epoch   2 |    50/ 2981 batches | lr 0.001 | ms/batch  6.30 | loss  5.41 | ppl   223.03\n epoch   2 |   100/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.22 | ppl   185.13\n epoch   2 |   150/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.22 | ppl   185.21\n epoch   2 |   200/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.21 | ppl   182.34\n epoch   2 |   250/ 2981 batches | lr 0.001 | ms/batch  6.46 | loss  5.21 | ppl   183.11\n epoch   2 |   300/ 2981 batches | lr 0.001 | ms/batch  6.12 | loss  5.28 | ppl   195.89\n epoch   2 |   350/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.22 | ppl   185.07\n epoch   2 |   400/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.21 | ppl   183.08\n epoch   2 |   450/ 2981 batches | lr 0.001 | ms/batch  6.19 | loss  5.02 | ppl   151.59\n epoch   2 |   500/ 2981 batches | lr 0.001 | ms/batch  6.20 | loss  5.16 | ppl   173.43\n epoch   2 |   550/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.02 | ppl   151.12\n epoch   2 |   600/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.98 | ppl   146.04\n epoch   2 |   650/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.10 | ppl   163.91\n epoch   2 |   700/ 2981 batches | lr 0.001 | ms/batch  6.16 | loss  5.07 | ppl   158.68\n epoch   2 |   750/ 2981 batches | lr 0.001 | ms/batch  6.16 | loss  5.07 | ppl   159.49\n epoch   2 |   800/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  5.17 | ppl   175.12\n epoch   2 |   850/ 2981 batches | lr 0.001 | ms/batch  6.11 | loss  5.08 | ppl   160.84\n epoch   2 |   900/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  5.04 | ppl   153.93\n epoch   2 |   950/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  5.08 | ppl   161.06\n epoch   2 |  1000/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.01 | ppl   150.21\n epoch   2 |  1050/ 2981 batches | lr 0.001 | ms/batch  6.18 | loss  5.09 | ppl   161.79\n epoch   2 |  1100/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.01 | ppl   149.81\n epoch   2 |  1150/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.04 | ppl   153.89\n epoch   2 |  1200/ 2981 batches | lr 0.001 | ms/batch  6.12 | loss  5.15 | ppl   172.20\n epoch   2 |  1250/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.13 | ppl   168.39\n epoch   2 |  1300/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.18 | ppl   177.93\n epoch   2 |  1350/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.07 | ppl   158.50\n epoch   2 |  1400/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.03 | ppl   152.91\n epoch   2 |  1450/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  5.11 | ppl   165.50\n epoch   2 |  1500/ 2981 batches | lr 0.001 | ms/batch  6.16 | loss  5.19 | ppl   179.85\n epoch   2 |  1550/ 2981 batches | lr 0.001 | ms/batch  6.30 | loss  5.16 | ppl   173.76\n epoch   2 |  1600/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.09 | ppl   162.29\n epoch   2 |  1650/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.00 | ppl   149.01\n epoch   2 |  1700/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.05 | ppl   156.75\n epoch   2 |  1750/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.06 | ppl   157.34\n epoch   2 |  1800/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.09 | ppl   163.00\n epoch   2 |  1850/ 2981 batches | lr 0.001 | ms/batch  6.31 | loss  5.14 | ppl   170.83\n epoch   2 |  1900/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.07 | ppl   159.97\n epoch   2 |  1950/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  5.04 | ppl   154.37\n epoch   2 |  2000/ 2981 batches | lr 0.001 | ms/batch  6.23 | loss  5.06 | ppl   158.12\n epoch   2 |  2050/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.03 | ppl   153.09\n epoch   2 |  2100/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.91 | ppl   136.30\n epoch   2 |  2150/ 2981 batches | lr 0.001 | ms/batch  6.24 | loss  4.88 | ppl   131.29\n epoch   2 |  2200/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  4.93 | ppl   138.00\n epoch   2 |  2250/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  4.92 | ppl   137.63\n epoch   2 |  2300/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.04 | ppl   153.72\n epoch   2 |  2350/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.09 | ppl   162.64\n epoch   2 |  2400/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  5.03 | ppl   152.59\n epoch   2 |  2450/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.97 | ppl   144.33\n epoch   2 |  2500/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  5.01 | ppl   149.87\n epoch   2 |  2550/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.04 | ppl   153.85\n epoch   2 |  2600/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  5.11 | ppl   166.37\n epoch   2 |  2650/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  5.01 | ppl   149.23\n epoch   2 |  2700/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.93 | ppl   139.05\n epoch   2 |  2750/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.98 | ppl   145.91\n epoch   2 |  2800/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  4.98 | ppl   145.75\n epoch   2 |  2850/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  4.98 | ppl   144.91\n epoch   2 |  2900/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  4.89 | ppl   133.37\n epoch   2 |  2950/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  4.85 | ppl   127.19\n-----------------------------------------------------------------------------------------\n end of epoch   2 | time: 39.32s | valid loss  5.26 | valid ppl   191.93\n-----------------------------------------------------------------------------------------\n epoch   3 |    50/ 2981 batches | lr 0.001 | ms/batch  6.25 | loss  5.00 | ppl   149.03\n epoch   3 |   100/ 2981 batches | lr 0.001 | ms/batch  6.52 | loss  4.84 | ppl   126.98\n epoch   3 |   150/ 2981 batches | lr 0.001 | ms/batch  6.12 | loss  4.83 | ppl   125.32\n epoch   3 |   200/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  4.85 | ppl   128.03\n epoch   3 |   250/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.87 | ppl   130.01\n epoch   3 |   300/ 2981 batches | lr 0.001 | ms/batch  6.16 | loss  4.93 | ppl   137.77\n epoch   3 |   350/ 2981 batches | lr 0.001 | ms/batch  6.24 | loss  4.86 | ppl   129.48\n epoch   3 |   400/ 2981 batches | lr 0.001 | ms/batch  6.25 | loss  4.87 | ppl   130.92\n epoch   3 |   450/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  4.67 | ppl   106.35\n epoch   3 |   500/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.80 | ppl   121.33\n epoch   3 |   550/ 2981 batches | lr 0.001 | ms/batch  6.26 | loss  4.68 | ppl   107.58\n epoch   3 |   600/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  4.67 | ppl   106.22\n epoch   3 |   650/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  4.76 | ppl   116.30\n epoch   3 |   700/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.73 | ppl   112.92\n epoch   3 |   750/ 2981 batches | lr 0.001 | ms/batch  6.17 | loss  4.76 | ppl   116.78\n epoch   3 |   800/ 2981 batches | lr 0.001 | ms/batch  6.17 | loss  4.86 | ppl   128.84\n epoch   3 |   850/ 2981 batches | lr 0.001 | ms/batch  6.16 | loss  4.78 | ppl   119.11\n epoch   3 |   900/ 2981 batches | lr 0.001 | ms/batch  6.25 | loss  4.73 | ppl   113.86\n epoch   3 |   950/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.78 | ppl   118.54\n epoch   3 |  1000/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  4.71 | ppl   110.59\n epoch   3 |  1050/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  4.78 | ppl   118.88\n epoch   3 |  1100/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.71 | ppl   111.54\n epoch   3 |  1150/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  4.74 | ppl   114.63\n epoch   3 |  1200/ 2981 batches | lr 0.001 | ms/batch  6.31 | loss  4.86 | ppl   128.39\n epoch   3 |  1250/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  4.84 | ppl   126.20\n epoch   3 |  1300/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  4.88 | ppl   131.85\n epoch   3 |  1350/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.77 | ppl   117.46\n epoch   3 |  1400/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  4.73 | ppl   112.86\n epoch   3 |  1450/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.80 | ppl   121.39\n epoch   3 |  1500/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.90 | ppl   133.63\n epoch   3 |  1550/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  4.87 | ppl   130.81\n epoch   3 |  1600/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.83 | ppl   124.80\n epoch   3 |  1650/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.72 | ppl   112.56\n epoch   3 |  1700/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.78 | ppl   119.54\n epoch   3 |  1750/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  4.79 | ppl   120.38\n epoch   3 |  1800/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.83 | ppl   124.92\n epoch   3 |  1850/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  4.86 | ppl   128.91\n epoch   3 |  1900/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.80 | ppl   122.08\n epoch   3 |  1950/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.77 | ppl   118.08\n epoch   3 |  2000/ 2981 batches | lr 0.001 | ms/batch  6.21 | loss  4.79 | ppl   120.70\n epoch   3 |  2050/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  4.75 | ppl   115.11\n epoch   3 |  2100/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.65 | ppl   104.07\n epoch   3 |  2150/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  4.60 | ppl    99.54\n epoch   3 |  2200/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  4.65 | ppl   104.69\n epoch   3 |  2250/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.65 | ppl   104.47\n epoch   3 |  2300/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  4.75 | ppl   115.96\n epoch   3 |  2350/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  4.82 | ppl   124.01\n epoch   3 |  2400/ 2981 batches | lr 0.001 | ms/batch  6.12 | loss  4.75 | ppl   115.41\n epoch   3 |  2450/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  4.71 | ppl   111.29\n epoch   3 |  2500/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.75 | ppl   115.32\n epoch   3 |  2550/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  4.77 | ppl   117.76\n epoch   3 |  2600/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.85 | ppl   128.15\n epoch   3 |  2650/ 2981 batches | lr 0.001 | ms/batch  6.13 | loss  4.74 | ppl   114.97\n epoch   3 |  2700/ 2981 batches | lr 0.001 | ms/batch  6.14 | loss  4.68 | ppl   108.00\n epoch   3 |  2750/ 2981 batches | lr 0.001 | ms/batch  6.12 | loss  4.71 | ppl   111.60\n epoch   3 |  2800/ 2981 batches | lr 0.001 | ms/batch  6.30 | loss  4.72 | ppl   111.71\n epoch   3 |  2850/ 2981 batches | lr 0.001 | ms/batch  6.19 | loss  4.73 | ppl   112.80\n epoch   3 |  2900/ 2981 batches | lr 0.001 | ms/batch  6.15 | loss  4.66 | ppl   105.22\n epoch   3 |  2950/ 2981 batches | lr 0.001 | ms/batch  6.17 | loss  4.61 | ppl   100.27\n-----------------------------------------------------------------------------------------\n end of epoch   3 | time: 39.11s | valid loss  5.25 | valid ppl   191.01\n-----------------------------------------------------------------------------------------\n</div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["####Evaluate the model with the test dataset\nThe `ort` model trained without ONNX Runtime accelerated is evaluated in the step below\n\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#evaluate-the-model-with-the-test-dataset for details."],"metadata":{}},{"cell_type":"code","source":["test_loss = evaluate(ort_model, test_data, accelerate_using_ort=True)\nprint('=' * 89)\nprint('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\nprint('=' * 89)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">=========================================================================================\n End of training | test loss  5.17 | test ppl   175.46\n=========================================================================================\n</div>"]}}],"execution_count":30},{"cell_type":"markdown","source":["###Summary\nThis notebook shows how to accelerate training of a `Transformer` model using `ONNX Runtime`. The optimizations on compute and memory utilization in `ONNX Runtime` **reduced the training time** of a simple sequence-to-sequence model by **~40%** (in each epoch and total training time) without any change in hyper parameters and without impacting training and test metrics on loss and ppl."],"metadata":{}},{"cell_type":"code","source":["%which python"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">UsageError: Line magic function `%which` not found.\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":33}],"metadata":{"name":"ATTv2","notebookId":2937200619401840},"nbformat":4,"nbformat_minor":0}
