{"cells":[{"cell_type":"markdown","source":["## Accelerated Training of Transformer Models\nThis notebook demonstrates using [ONNX Runtime](https://cloudblogs.microsoft.com/opensource/2020/05/19/announcing-support-for-accelerated-training-with-onnx-runtime/) to accelerate the training of a sequence-to-sequence model. It uses the implementation available at [PyTorch tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) which uses the [nn.Transformer](https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer) module. This notebook is available at [https://github.com/skaarthik/onnxruntime-training-databricks](https://github.com/skaarthik/onnxruntime-training-databricks). Additional examples on pretraining and finetuning involving BERT and GPT-2 are available at [https://github.com/microsoft/onnxruntime-training-examples](https://github.com/microsoft/onnxruntime-training-examples).\n\nNotes: \n* Custom containers are not supported on ML Runtime in Databricks. This demo needs ML Runtime and GPUs. As an alternative to packaging dependencies in a customer container, refer to the `Prerequisites` section below to prepare Databricks environment for this demo.\n* `%load` and `%run` commands are not supported in Databricks notebooks (see [Databricks Notebooks documentation](https://docs.databricks.com/notebooks/notebooks-use.html#run-a-notebook-from-another-notebook) for details). Due to this limitation, this notebook has the Python scripts included instead of loading directly from the GitHub repo."],"metadata":{}},{"cell_type":"markdown","source":["####Prerequisites for the demo\nRun the following commands to install the following packages to the Conda environment you will be using:\n* pip install torchtext\n* pip install onnx\n* pip install cerberus\n* pip install https://onnxtraining.blob.core.windows.net/ort-databricks-demo/onnxruntime_gpu-1.5.1-cp37-cp37m-linux_x86_64.whl"],"metadata":{}},{"cell_type":"markdown","source":["####Define the model\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model for details."],"metadata":{}},{"cell_type":"code","source":["import math\nimport torch\nimport torch.nn as nn\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\nclass TransformerModel(nn.Module):\n\n    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n        super(TransformerModel, self).__init__()\n        self.model_type = 'Transformer'\n        self.src_mask = None\n        self.pos_encoder = PositionalEncoding(ninp, dropout)\n        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        self.ninp = ninp\n        self.decoder = nn.Linear(ninp, ntoken)\n\n        self.init_weights()\n\n    def _generate_square_subsequent_mask(self, src):\n        sz = src.size(0)\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src):\n        if self.src_mask == None or self.src_mask.size(0) != src.size(0):\n            device = src.device\n            mask = self._generate_square_subsequent_mask(src).to(device)\n            self.src_mask = mask\n\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, self.src_mask)\n        output = self.decoder(output)\n        return output"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["####Load and batch data\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#load-and-batch-data for details."],"metadata":{}},{"cell_type":"code","source":["import torchtext\nfrom torchtext.data.utils import get_tokenizer\nTEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n                            init_token='<sos>',\n                            eos_token='<eos>',\n                            lower=True)\ntrain_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\nTEXT.build_vocab(train_txt)\ndevice = torch.device(\"cuda\")"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n  warnings.warn(&#39;{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.&#39;.format(self.__class__.__name__), UserWarning)\n/databricks/python/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n  warnings.warn(&#39;Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.&#39;, UserWarning)\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["####Functions to generate input and target sequence\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#functions-to-generate-input-and-target-sequence for details."],"metadata":{}},{"cell_type":"code","source":["bptt = 35\ndef get_batch(source, i):\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].view(-1)\n    return data, target\n  \ndef batchify(data, bsz):\n    data = TEXT.numericalize([data.examples[0].text])\n    # Divide the dataset into bsz parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    data = data.view(bsz, -1).t().contiguous()\n    return data.to(device)\n\nbatch_size = 20\neval_batch_size = 10\ntrain_data = batchify(train_txt, batch_size)\nval_data = batchify(val_txt, eval_batch_size)\ntest_data = batchify(test_txt, eval_batch_size)  "],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["####Initiate an instance\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#initiate-an-instance for details."],"metadata":{}},{"cell_type":"code","source":["ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\nemsize = 200 # embedding dimension\nnhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\nnlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\nnhead = 2 # the number of heads in the multiheadattention models\ndropout = 0.2 # the dropout value\nlr = 0.001\n\ndef calculate_loss(output, targets):\n    output = output.view(-1, len(TEXT.vocab.stoi))\n    return criterion(output, targets) "],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\ncriterion = nn.CrossEntropyLoss()\nlearning_rate = lr\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nmetric_prefix = 'baseline'  "],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["import mlflow\ndef log_metrics(epoch, batch, train_data_len, bptt, lr, elapsed, log_interval, cur_loss, log_prefix):\n  print('| epoch {:3d} | {:5d}/{:5d} batches | '\n        'lr {:02.3f} | ms/batch {:5.2f} | '\n        'loss {:5.2f} | ppl {:8.2f}'.format(\n          epoch, batch, train_data_len // bptt, lr,\n          elapsed * 1000 / log_interval,\n          cur_loss, math.exp(cur_loss)))\n  mlflow.log_metric(log_prefix + '_milliseconds/batch', elapsed * 1000 / log_interval, step=batch)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["####Define `train` and `evaluate` methods\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#run-the-model for details."],"metadata":{}},{"cell_type":"code","source":["import time\ndef train(epoch, accelerate_using_ort):\n    if not accelerate_using_ort:\n      model.train() # Turn on the train mode\n      \n    total_loss = 0.\n    start_time = time.time()\n\n    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n        data, targets = get_batch(train_data, i)\n        \n        if not accelerate_using_ort:\n          optimizer.zero_grad()\n          output = model(data)\n          loss = calculate_loss(output, targets)\n          loss.backward()\n          torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n          optimizer.step()\n          current_learning_rate = learning_rate\n        else:\n          loss, output = trainer.train_step(data, targets)\n          current_learning_rate = learning_rate\n\n        total_loss += loss.item()\n        log_interval = 50\n\n        if batch % log_interval == 0 and batch > 0:\n            cur_loss = total_loss / log_interval\n            elapsed = time.time() - start_time\n\n            log_metrics(epoch, \n                        batch, \n                        len(train_data),\n                        bptt, \n                        current_learning_rate,\n                        elapsed,\n                        log_interval,\n                        cur_loss,\n                        metric_prefix)\n            \n            total_loss = 0\n            start_time = time.time()           "],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["def evaluate(eval_model, data_source, accelerate_using_ort):\n    if not accelerate_using_ort:\n      eval_model.eval() # Turn on the evaluation mode\n      \n    total_loss = 0.\n    ntokens = len(TEXT.vocab.stoi)\n    with torch.no_grad():\n        for i in range(0, data_source.size(0) - 1, bptt):\n            data, targets = get_batch(data_source, i)\n            if not accelerate_using_ort:\n              output = eval_model(data)\n              output_flat = output.view(-1, ntokens)\n              total_loss += len(data) * criterion(output_flat, targets).item()\n            else:\n              loss, outputs = trainer.eval_step(data, targets)\n              total_loss += len(data) * loss.item()              \n            \n    return total_loss / (len(data_source) - 1)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["####Train the model\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#run-the-model for details."],"metadata":{}},{"cell_type":"code","source":["def train_model(accelerate_using_ort=False):\n  best_val_loss = float(\"inf\")\n  epochs = 3 # The number of epochs\n  best_model = None\n\n  for epoch in range(1, epochs + 1):\n      epoch_start_time = time.time()\n      \n      train(epoch, accelerate_using_ort)\n        \n      val_loss = evaluate(model, val_data, accelerate_using_ort)\n      print('-' * 89)\n      print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n            'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                       val_loss, math.exp(val_loss)))\n      print('-' * 89)\n\n      if val_loss < best_val_loss:\n          best_val_loss = val_loss\n          best_model = model\n  return best_model"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["baseline_model = train_model(accelerate_using_ort=False)"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">| epoch   1 |    50/ 2981 batches | lr 0.001 | ms/batch 16.92 | loss  7.73 | ppl  2265.90\n epoch   1 |   100/ 2981 batches | lr 0.001 | ms/batch 12.12 | loss  6.70 | ppl   811.65\n epoch   1 |   150/ 2981 batches | lr 0.001 | ms/batch 12.47 | loss  6.50 | ppl   667.39\n epoch   1 |   200/ 2981 batches | lr 0.001 | ms/batch 12.30 | loss  6.35 | ppl   572.15\n epoch   1 |   250/ 2981 batches | lr 0.001 | ms/batch 12.40 | loss  6.26 | ppl   523.21\n epoch   1 |   300/ 2981 batches | lr 0.001 | ms/batch 12.14 | loss  6.22 | ppl   503.98\n epoch   1 |   350/ 2981 batches | lr 0.001 | ms/batch 12.13 | loss  6.09 | ppl   443.11\n epoch   1 |   400/ 2981 batches | lr 0.001 | ms/batch 12.12 | loss  6.07 | ppl   433.20\n epoch   1 |   450/ 2981 batches | lr 0.001 | ms/batch 12.23 | loss  5.93 | ppl   377.42\n epoch   1 |   500/ 2981 batches | lr 0.001 | ms/batch 12.22 | loss  6.02 | ppl   409.71\n epoch   1 |   550/ 2981 batches | lr 0.001 | ms/batch 12.43 | loss  5.88 | ppl   359.41\n epoch   1 |   600/ 2981 batches | lr 0.001 | ms/batch 12.28 | loss  5.81 | ppl   333.39\n epoch   1 |   650/ 2981 batches | lr 0.001 | ms/batch 12.18 | loss  5.93 | ppl   377.16\n epoch   1 |   700/ 2981 batches | lr 0.001 | ms/batch 12.28 | loss  5.84 | ppl   343.27\n epoch   1 |   750/ 2981 batches | lr 0.001 | ms/batch 12.22 | loss  5.78 | ppl   324.97\n epoch   1 |   800/ 2981 batches | lr 0.001 | ms/batch 12.37 | loss  5.88 | ppl   358.71\n epoch   1 |   850/ 2981 batches | lr 0.001 | ms/batch 12.23 | loss  5.78 | ppl   322.77\n epoch   1 |   900/ 2981 batches | lr 0.001 | ms/batch 12.20 | loss  5.74 | ppl   309.91\n epoch   1 |   950/ 2981 batches | lr 0.001 | ms/batch 12.47 | loss  5.78 | ppl   323.99\n epoch   1 |  1000/ 2981 batches | lr 0.001 | ms/batch 12.22 | loss  5.72 | ppl   303.70\n epoch   1 |  1050/ 2981 batches | lr 0.001 | ms/batch 12.25 | loss  5.77 | ppl   321.75\n epoch   1 |  1100/ 2981 batches | lr 0.001 | ms/batch 12.25 | loss  5.69 | ppl   295.88\n epoch   1 |  1150/ 2981 batches | lr 0.001 | ms/batch 12.21 | loss  5.69 | ppl   295.65\n epoch   1 |  1200/ 2981 batches | lr 0.001 | ms/batch 12.30 | loss  5.79 | ppl   328.18\n epoch   1 |  1250/ 2981 batches | lr 0.001 | ms/batch 12.18 | loss  5.75 | ppl   312.72\n epoch   1 |  1300/ 2981 batches | lr 0.001 | ms/batch 12.21 | loss  5.81 | ppl   334.17\n epoch   1 |  1350/ 2981 batches | lr 0.001 | ms/batch 12.29 | loss  5.69 | ppl   295.12\n epoch   1 |  1400/ 2981 batches | lr 0.001 | ms/batch 12.29 | loss  5.65 | ppl   283.23\n epoch   1 |  1450/ 2981 batches | lr 0.001 | ms/batch 12.25 | loss  5.75 | ppl   313.70\n epoch   1 |  1500/ 2981 batches | lr 0.001 | ms/batch 12.45 | loss  5.78 | ppl   322.31\n epoch   1 |  1550/ 2981 batches | lr 0.001 | ms/batch 12.29 | loss  5.72 | ppl   304.06\n epoch   1 |  1600/ 2981 batches | lr 0.001 | ms/batch 12.30 | loss  5.65 | ppl   284.63\n epoch   1 |  1650/ 2981 batches | lr 0.001 | ms/batch 12.28 | loss  5.59 | ppl   268.38\n epoch   1 |  1700/ 2981 batches | lr 0.001 | ms/batch 12.30 | loss  5.61 | ppl   274.10\n epoch   1 |  1750/ 2981 batches | lr 0.001 | ms/batch 12.29 | loss  5.60 | ppl   269.77\n epoch   1 |  1800/ 2981 batches | lr 0.001 | ms/batch 12.22 | loss  5.63 | ppl   277.75\n epoch   1 |  1850/ 2981 batches | lr 0.001 | ms/batch 12.48 | loss  5.68 | ppl   292.29\n epoch   1 |  1900/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  5.61 | ppl   273.17\n epoch   1 |  1950/ 2981 batches | lr 0.001 | ms/batch 12.20 | loss  5.58 | ppl   264.36\n epoch   1 |  2000/ 2981 batches | lr 0.001 | ms/batch 12.14 | loss  5.58 | ppl   265.90\n epoch   1 |  2050/ 2981 batches | lr 0.001 | ms/batch 12.31 | loss  5.57 | ppl   262.43\n epoch   1 |  2100/ 2981 batches | lr 0.001 | ms/batch 12.34 | loss  5.47 | ppl   236.78\n epoch   1 |  2150/ 2981 batches | lr 0.001 | ms/batch 12.52 | loss  5.41 | ppl   224.60\n epoch   1 |  2200/ 2981 batches | lr 0.001 | ms/batch 12.54 | loss  5.46 | ppl   235.69\n epoch   1 |  2250/ 2981 batches | lr 0.001 | ms/batch 12.43 | loss  5.47 | ppl   237.23\n epoch   1 |  2300/ 2981 batches | lr 0.001 | ms/batch 12.34 | loss  5.56 | ppl   260.50\n epoch   1 |  2350/ 2981 batches | lr 0.001 | ms/batch 12.54 | loss  5.63 | ppl   277.49\n epoch   1 |  2400/ 2981 batches | lr 0.001 | ms/batch 12.45 | loss  5.55 | ppl   257.84\n epoch   1 |  2450/ 2981 batches | lr 0.001 | ms/batch 12.29 | loss  5.50 | ppl   245.87\n epoch   1 |  2500/ 2981 batches | lr 0.001 | ms/batch 12.68 | loss  5.53 | ppl   251.61\n epoch   1 |  2550/ 2981 batches | lr 0.001 | ms/batch 12.31 | loss  5.55 | ppl   257.03\n epoch   1 |  2600/ 2981 batches | lr 0.001 | ms/batch 12.29 | loss  5.60 | ppl   269.41\n epoch   1 |  2650/ 2981 batches | lr 0.001 | ms/batch 12.62 | loss  5.45 | ppl   232.52\n epoch   1 |  2700/ 2981 batches | lr 0.001 | ms/batch 12.17 | loss  5.38 | ppl   216.26\n epoch   1 |  2750/ 2981 batches | lr 0.001 | ms/batch 12.33 | loss  5.49 | ppl   241.72\n epoch   1 |  2800/ 2981 batches | lr 0.001 | ms/batch 12.04 | loss  5.45 | ppl   232.52\n epoch   1 |  2850/ 2981 batches | lr 0.001 | ms/batch 12.39 | loss  5.45 | ppl   233.74\n epoch   1 |  2900/ 2981 batches | lr 0.001 | ms/batch 12.32 | loss  5.36 | ppl   211.90\n epoch   1 |  2950/ 2981 batches | lr 0.001 | ms/batch 12.16 | loss  5.29 | ppl   198.84\n-----------------------------------------------------------------------------------------\n end of epoch   1 | time: 46.78s | valid loss  5.36 | valid ppl   212.77\n-----------------------------------------------------------------------------------------\n epoch   2 |    50/ 2981 batches | lr 0.001 | ms/batch 12.51 | loss  5.39 | ppl   219.65\n epoch   2 |   100/ 2981 batches | lr 0.001 | ms/batch 12.21 | loss  5.19 | ppl   178.93\n epoch   2 |   150/ 2981 batches | lr 0.001 | ms/batch 12.12 | loss  5.21 | ppl   182.69\n epoch   2 |   200/ 2981 batches | lr 0.001 | ms/batch 12.19 | loss  5.19 | ppl   179.01\n epoch   2 |   250/ 2981 batches | lr 0.001 | ms/batch 12.24 | loss  5.21 | ppl   182.30\n epoch   2 |   300/ 2981 batches | lr 0.001 | ms/batch 12.12 | loss  5.28 | ppl   195.86\n epoch   2 |   350/ 2981 batches | lr 0.001 | ms/batch 12.12 | loss  5.21 | ppl   182.23\n epoch   2 |   400/ 2981 batches | lr 0.001 | ms/batch 12.13 | loss  5.20 | ppl   181.59\n epoch   2 |   450/ 2981 batches | lr 0.001 | ms/batch 12.24 | loss  5.02 | ppl   152.16\n epoch   2 |   500/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  5.15 | ppl   172.33\n epoch   2 |   550/ 2981 batches | lr 0.001 | ms/batch 12.15 | loss  5.01 | ppl   149.53\n epoch   2 |   600/ 2981 batches | lr 0.001 | ms/batch 12.23 | loss  4.97 | ppl   144.52\n epoch   2 |   650/ 2981 batches | lr 0.001 | ms/batch 12.19 | loss  5.10 | ppl   164.38\n epoch   2 |   700/ 2981 batches | lr 0.001 | ms/batch 12.24 | loss  5.06 | ppl   158.04\n epoch   2 |   750/ 2981 batches | lr 0.001 | ms/batch 12.16 | loss  5.06 | ppl   157.17\n epoch   2 |   800/ 2981 batches | lr 0.001 | ms/batch 12.48 | loss  5.15 | ppl   172.97\n epoch   2 |   850/ 2981 batches | lr 0.001 | ms/batch 12.55 | loss  5.06 | ppl   156.91\n epoch   2 |   900/ 2981 batches | lr 0.001 | ms/batch 12.10 | loss  5.02 | ppl   151.90\n epoch   2 |   950/ 2981 batches | lr 0.001 | ms/batch 12.22 | loss  5.07 | ppl   158.63\n epoch   2 |  1000/ 2981 batches | lr 0.001 | ms/batch 12.18 | loss  5.00 | ppl   148.51\n epoch   2 |  1050/ 2981 batches | lr 0.001 | ms/batch 12.39 | loss  5.07 | ppl   159.86\n epoch   2 |  1100/ 2981 batches | lr 0.001 | ms/batch 12.19 | loss  4.99 | ppl   147.51\n epoch   2 |  1150/ 2981 batches | lr 0.001 | ms/batch 12.24 | loss  5.02 | ppl   150.67\n epoch   2 |  1200/ 2981 batches | lr 0.001 | ms/batch 12.28 | loss  5.13 | ppl   168.25\n epoch   2 |  1250/ 2981 batches | lr 0.001 | ms/batch 12.24 | loss  5.13 | ppl   168.75\n epoch   2 |  1300/ 2981 batches | lr 0.001 | ms/batch 12.45 | loss  5.16 | ppl   174.67\n epoch   2 |  1350/ 2981 batches | lr 0.001 | ms/batch 12.08 | loss  5.05 | ppl   155.44\n epoch   2 |  1400/ 2981 batches | lr 0.001 | ms/batch 12.35 | loss  5.02 | ppl   151.20\n epoch   2 |  1450/ 2981 batches | lr 0.001 | ms/batch 12.27 | loss  5.10 | ppl   163.87\n epoch   2 |  1500/ 2981 batches | lr 0.001 | ms/batch 12.21 | loss  5.18 | ppl   177.61\n epoch   2 |  1550/ 2981 batches | lr 0.001 | ms/batch 12.31 | loss  5.14 | ppl   170.41\n epoch   2 |  1600/ 2981 batches | lr 0.001 | ms/batch 12.14 | loss  5.07 | ppl   159.06\n epoch   2 |  1650/ 2981 batches | lr 0.001 | ms/batch 12.09 | loss  4.99 | ppl   146.23\n epoch   2 |  1700/ 2981 batches | lr 0.001 | ms/batch 12.20 | loss  5.04 | ppl   153.75\n epoch   2 |  1750/ 2981 batches | lr 0.001 | ms/batch 12.11 | loss  5.04 | ppl   154.32\n epoch   2 |  1800/ 2981 batches | lr 0.001 | ms/batch 12.29 | loss  5.08 | ppl   160.49\n epoch   2 |  1850/ 2981 batches | lr 0.001 | ms/batch 12.18 | loss  5.12 | ppl   166.99\n epoch   2 |  1900/ 2981 batches | lr 0.001 | ms/batch 12.14 | loss  5.06 | ppl   157.50\n epoch   2 |  1950/ 2981 batches | lr 0.001 | ms/batch 12.41 | loss  5.02 | ppl   152.03\n epoch   2 |  2000/ 2981 batches | lr 0.001 | ms/batch 12.26 | loss  5.04 | ppl   153.81\n epoch   2 |  2050/ 2981 batches | lr 0.001 | ms/batch 12.18 | loss  5.00 | ppl   148.26\n epoch   2 |  2100/ 2981 batches | lr 0.001 | ms/batch 12.30 | loss  4.89 | ppl   133.50\n epoch   2 |  2150/ 2981 batches | lr 0.001 | ms/batch 12.19 | loss  4.85 | ppl   128.27\n epoch   2 |  2200/ 2981 batches | lr 0.001 | ms/batch 12.23 | loss  4.91 | ppl   135.64\n epoch   2 |  2250/ 2981 batches | lr 0.001 | ms/batch 12.16 | loss  4.90 | ppl   133.91\n epoch   2 |  2300/ 2981 batches | lr 0.001 | ms/batch 12.11 | loss  5.01 | ppl   149.57\n</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["####Evaluate the model with the test dataset\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#evaluate-the-model-with-the-test-dataset for details."],"metadata":{}},{"cell_type":"code","source":["test_loss = evaluate(baseline_model, test_data)\nprint('=' * 89)\nprint('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n    test_loss, math.exp(test_loss)))\nprint('=' * 89)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["####Define `train_in_ort` method\nRefer to https://github.com/microsoft/onnxruntime-training-examples/tree/master/getting-started for details on changes needed to accelerate a PyTorch implementation using ONNX Runtime (ORT)."],"metadata":{}},{"cell_type":"code","source":["from onnxruntime.training import ORTTrainer, optim\n\nmodel_description = {'inputs':  [('src', ['bptt', 'batch_size']),\n                                 ('label', ['bptt_x_batch_size'])],\n                     'outputs': [('loss', [], True),\n                                 ('output', ['bptt', 'batch_size', ntokens])]}\nmodel = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\ncriterion = nn.CrossEntropyLoss()\nlearning_rate = lr\noptimizer_config = optim.AdamConfig(lr=learning_rate)\ntrainer = ORTTrainer(model,               # model\n                     model_description,   # model description\n                     optimizer_config,    # optimizer configuration\n                     calculate_loss)      # loss function\nmetric_prefix = 'onnxruntime'"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["ort_model = train_model(accelerate_using_ort=True)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["####Evaluate the model with the test dataset\nRefer to https://pytorch.org/tutorials/beginner/transformer_tutorial.html#evaluate-the-model-with-the-test-dataset for details."],"metadata":{}},{"cell_type":"code","source":["test_loss = evaluate(ort_model, test_data)\nprint('=' * 89)\nprint('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n    test_loss, math.exp(test_loss)))\nprint('=' * 89)"],"metadata":{},"outputs":[],"execution_count":27}],"metadata":{"name":"ATTv2","notebookId":1979148581943929},"nbformat":4,"nbformat_minor":0}
